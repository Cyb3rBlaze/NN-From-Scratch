{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils.loss import binary_cross_entropy_loss, d_binary_cross_entropy_loss\n",
    "from utils.number import Number\n",
    "from utils.activation import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, num_prev_neurons, num_neurons, activation=\"linear\"):\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_prev_neurons = num_prev_neurons\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.weights = np.random.randn(num_prev_neurons, num_neurons)\n",
    "        self.biases = np.array([Number(i-0.5) for i in np.random.rand(num_neurons)])\n",
    "    \n",
    "    def print_params(self):\n",
    "        print(\"WEIGHTS:\")\n",
    "        print(self.weights)\n",
    "        print(\"BIASES:\")\n",
    "        print(self.biases)\n",
    "\n",
    "    def forward(self, input):\n",
    "        lin = input @ self.weights + self.biases\n",
    "\n",
    "        # storing original input passes for chain rule operations during backprop\n",
    "        self.original_input = input\n",
    "        self.lin_pass = lin\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return sigmoid(lin)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return tanh(lin)\n",
    "        return lin\n",
    "\n",
    "    def backward(self, prev_chain, lr):\n",
    "        ''' chain rule:\n",
    "        dz1_db = 1\n",
    "        dz1_dw = original_input\n",
    "\n",
    "        dout_dw = prev_chain*da1_dz1*dz1_dw -- prev_chain = dout_da2*da2_dz2*dz2_da1\n",
    "        dout_db = prev_chain*da1_dz1*dz1_db -- prev_chain = dout_da2*da2_dz2*dz2_da1\n",
    "\n",
    "        prev_chain is an array representing the chain rule computed values so far in the subsequent layer\n",
    "        e.g.\n",
    "        dout_dw1 = dout_dw3*dw3_dw1 + dout_dw4*dw4_dw1\n",
    "        dout_db1 = dout_db3*db3_db1 + dout_db4*dw4_db1\n",
    "        prev_chain would be [[dout_dw3, dout_dw4], [dout_db3, dout_db4]]\n",
    "        '''\n",
    "        da = lambda x: x\n",
    "        if self.activation == \"sigmoid\":\n",
    "            da = d_sigmoid\n",
    "        elif self.activation == \"tanh\":\n",
    "            da = d_tanh\n",
    "        \n",
    "        prev_chain_dw, prev_chain_db = prev_chain\n",
    "\n",
    "        dout_dw =  (np.tile(self.original_input, (self.num_neurons, 1)).reshape((self.num_prev_neurons, -1)) @ (prev_chain_dw * da(self.lin_pass))).reshape((self.num_prev_neurons, self.num_neurons))\n",
    "        dout_db = np.dot(np.tile(da(self.lin_pass), (prev_chain_db.shape[0], 1)).T, prev_chain_db)\n",
    "\n",
    "        self.weights -= lr * dout_dw\n",
    "        self.biases -= lr * dout_db\n",
    "\n",
    "        ''' returning new prev_chain:\n",
    "        output = a2(z2(a1(z1(input)))) -- e.g. two activation + linear operations\n",
    "\n",
    "        dout_dw = dout_da2*da2_dz2*dz2_da1*da1_dz1*dz1_dw\n",
    "        dout_dw = prev_chain*da1_dz1*dz1_dw -- prev_chain = dout_da2*da2_dz2*dz2_da1\n",
    "\n",
    "        dout_db = dout_da2*da2_dz2*dz2_da1*da1_dz1*dz1_db\n",
    "        dout_db = prev_chain*da1_dz1*dz1_db -- prev_chain = dout_da2*da2_dz2*dz2_da1\n",
    "        '''\n",
    "        return (dout_dw, dout_db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train step function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining train step function for gradient descent\n",
    "\n",
    "def forward_pass(model, input):\n",
    "    logit = input\n",
    "    for i in range(len(model)):\n",
    "        logit = model[i].forward(logit)\n",
    "    return logit\n",
    "\n",
    "def train_step(model, input, label, lr):\n",
    "    logit = forward_pass(model, input)\n",
    "\n",
    "    loss = binary_cross_entropy_loss(logit, label)\n",
    "    # duplicating to update weights + biases\n",
    "    prev_chain = np.tile(d_binary_cross_entropy_loss(logit, label), (2, 1))\n",
    "\n",
    "    for i in range(len(model)):\n",
    "        prev_chain = model[len(model)-i-1].backward(prev_chain, lr)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One neuron perceptron test\n",
    "\n",
    "### NOT gate task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4375142632327579\n",
      "Loss: 0.34176956427860294\n",
      "Loss: 0.23621155401343397\n",
      "Loss: 0.1770863782685856\n",
      "Loss: 0.14052307819664078\n",
      "Loss: 0.1159917996506304\n",
      "Loss: 0.09850929560680108\n",
      "Loss: 0.0854713783894216\n",
      "Loss: 0.0754005490306617\n",
      "Loss: 0.06740179536363063\n",
      "[0.03895825]\n",
      "[0.9409139]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 0]\n",
    "labels = [0, 1]\n",
    "\n",
    "model = [Layer(1, 1, activation=\"sigmoid\")]\n",
    "\n",
    "real_input = np.array([inputs[0]])\n",
    "real_label = np.array([labels[0]])\n",
    "\n",
    "for i in range(1000):\n",
    "    loss = train_step(model, real_input, real_label, 0.1)\n",
    "\n",
    "    real_input = np.array([inputs[i%2]])\n",
    "    real_label = np.array([labels[i%2]])\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"Loss: \" + str(loss))\n",
    "\n",
    "print(forward_pass(model, np.array([inputs[0]])))\n",
    "print(forward_pass(model, np.array([inputs[1]])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron network test\n",
    "### NOT gate task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.13368054873893712\n",
      "Loss: 0.5359913095532125\n",
      "Loss: 0.4616217827486787\n",
      "Loss: 0.257270966902606\n",
      "Loss: 0.14387368644854448\n",
      "Loss: 0.08903602230913465\n",
      "Loss: 0.06084829464398274\n",
      "Loss: 0.04480855692513374\n",
      "Loss: 0.034825973867756044\n",
      "Loss: 0.02815834348847625\n",
      "[0.06173959]\n",
      "[0.9768192]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 0]\n",
    "labels = [0, 1]\n",
    "\n",
    "model = [Layer(1, 10, activation=\"sigmoid\"), Layer(10, 10, activation=\"sigmoid\"), Layer(10, 1, activation=\"sigmoid\")]\n",
    "\n",
    "real_input = np.array([inputs[0]])\n",
    "real_label = np.array([labels[0]])\n",
    "\n",
    "for i in range(1000):\n",
    "    loss = train_step(model, real_input, real_label, 0.1)\n",
    "\n",
    "    real_input = np.array([inputs[i%2]])\n",
    "    real_label = np.array([labels[i%2]])\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"Loss: \" + str(loss))\n",
    "\n",
    "print(forward_pass(model, np.array([inputs[0]])))\n",
    "print(forward_pass(model, np.array([inputs[1]])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "503745c8e30f479c4402e22a289640c4032d9a270f8b982fab510fb648d15b8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
